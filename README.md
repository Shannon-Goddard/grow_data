![header_pic](/Resources/pics/header_pic.png)
 
#### Table of Contents  

[Project Overview](#project-overview)  
[Resources](#resources)  
[Objectives](#objectives)  
[Summary](#summary)  
[Limitations](#limitations)  
  
## Project Overview  
In this module, we created a .csv file with 2,793 rows of data for each strain listed on the wikileaf website. Using Python in Jupyter Notebook to test code and Pandas with Beautiful Soup to scrape the data.

## Resources  
- **Software:** VS Code, Jupyter Notebook   
- **Languages:** Python  
- **Data Source:** [wikileaf.com](https://www.wikileaf.com/strains/)    

## Objectives  
- Import, analyze, clean, and preprocess a “real-world” classification dataset    

## Summary
**View the raw .csv [here](https://raw.githubusercontent.com/Shannon-Goddard/grow_data/main/Resources/csv/ALL_data.csv)**  

*Coming soon...*  
**Check-out this interactive website this data was used with [here]()**  
**Check-out the app this data was used with [here]()**

![](/Resources/pics/gif.gif)  

## Limitations  
Not all the individual data websites were able to be executed within the Jupyter Notebook due to missing letters of the strain names on individual website. Had to mannually go into .csv file "Find" and replace as code ran and crashed with printed ammended lines leading to name of strain prior to '404'. | Shannon-Goddard/grow_data is licensed under the
MIT License A short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.**Permissible** [MIT License](https://github.com/Shannon-Goddard/grow_data/blob/main/LICENSE)
