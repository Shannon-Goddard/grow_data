![header_pic](/Resources/pics/header_pic.png)
 
#### Table of Contents  

[Project Overview](#project-overview)  
[Resources](#resources)  
[Objectives](#objectives)  
[Summary](#summary)  
[Limitations](#limitations)  
  
## Project Overview  
In this module, we created a .csv file with 2,793 rows of data for each strain listed on the wikileaf website. Using Python in Jupyter Notebook to test code and Pandas with Beautiful Soup to scrape the data.

## Resources  
- **Software:** VS Code, Jupyter Notebook   
- **Languages:** Python  
- **Data Source:** [wikileaf.com](https://www.wikileaf.com/strains/)    

## Objectives  
- Import, analyze, clean, and preprocess a “real-world” classification dataset    

## Summary
**Vie the raw .csv [here](https://raw.githubusercontent.com/Shannon-Goddard/grow_data/main/Resources/csv/ALL_data.csv)**  

*Coming soon...*
**Check-out this interactive website this data was used with [here]()**  
**Check-out the app this data was used with [here]()**

![](/Resources/pics/gif.png)  

## Limitations  
**Not all the individual data websites were able to be executed within the Jupyter Notebook due to missing letter of the strain names on website. Had to mannually go into .csv file "Find" and replace.** **| Permissible** [MIT License](https://github.com/Shannon-Goddard/grow_data/blob/main/LICENSE)
